## 📋 목차
- [pre trained 모델이란?](#-pre-trained-모델의-핵심-개념)
- [Qwen3 토크나이저 사용법](#-qwen3-토크나이저-사용법)
- [Gemma 3 토크나이저 사용법](#-gemma-3의-주요-특징)
- [GitHub 리포지토리 상단 메뉴 기능 완전 가이드](#-github-리포지토리-상단-메뉴-기능-완전-가이드)



사전 훈련된(Pre-trained) 모델은 **대규모 데이터셋을 사용하여 미리 훈련시킨 딥러닝 모델**을 의미합니다[1]. 이는 특정 작업을 수행하기 위해 방대한 양의 데이터로 학습을 완료하여 이미 출력 도출이 가능한 상태의 모델입니다[3].

## 🎯 **Pre-trained 모델의 핵심 개념**

Pre-trained 모델은 말 그대로 "사전 훈련"을 의미하며, 모델이 다양한 패턴, 특징, 언어 구조 등을 학습할 수 있도록 대규모 범용 데이터셋으로 미리 훈련된 상태입니다[1]. 예를 들어:

- **이미지 인식**: 객체, 색상, 형태 등 다양한 시각적 특징 학습
- **자연어 처리**: 단어, 문장 구조, 문맥 등 언어의 다양한 요소 학습[1]

## 🔄 **Pre-trained 모델의 활용 과정**

### **1단계: Pre-training (사전 훈련)**
- 대규모 범용 데이터셋을 사용한 초기 훈련
- 기본적인 패턴과 특징들을 학습[1]

### **2단계: Fine-tuning (미세 조정)**
- 사전 훈련된 모델을 특정 작업에 맞게 조정
- 상대적으로 작은 특화된 데이터셋 사용
- 해당 작업에 최적화[1][2]

## 💡 **Pre-trained 모델의 필요성과 장점**

### **데이터 부족 문제 해결**
모든 작업에 대해 충분하고 특화된 데이터를 확보하는 것은 현실적으로 어렵습니다. Pre-trained 모델은 이러한 문제의 해결책으로 등장했습니다[1].

### **효율성과 경제성**
- **시간 절약**: 처음부터 학습할 필요 없음
- **비용 절감**: 천문학적인 학습 비용과 시간 단축[5]
- **인력 절약**: 데이터 전처리 및 정제 과정 단축[5]

### **성능 향상**
- **Low-level 특성 활용**: 선, 점, 곡선 같은 기본적인 특징들을 이미 학습[3]
- **적은 데이터로도 높은 성능**: Transfer learning을 통해 효과 극대화[3]

## 🌐 **대표적인 Pre-trained 모델 예시**

- **ImageNet**: 이미지 분류 작업에 널리 사용
- **BERT**: 자연어 처리 분야의 대표 모델
- **GPT 시리즈**: 대화형 AI 및 텍스트 생성[3][5]

## 🔧 **실제 활용 방법**

Pre-trained 모델은 **Hugging Face**와 같은 플랫폼에서 쉽게 접근할 수 있으며, 다양한 개발자들이 학습시킨 모델들을 공유하고 있습니다[5]. 사용자는 이러한 모델을 다운로드받아 자신의 특정 작업에 맞게 Fine-tuning하여 활용할 수 있습니다.

Pre-trained 모델은 현대 AI 개발에서 **필수적인 출발점**이 되었으며, 개발자들이 복잡한 AI 시스템을 더 효율적으로 구축할 수 있게 해주는 핵심 기술입니다.



## 🔧 **Qwen3 토크나이저 사용법**

### **기본 토크나이저 로딩**
```python
from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "Qwen/Qwen3-8B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
```

### **Transformers 버전 요구사항**
Qwen3 토크나이저를 사용하려면 **`transformers>=4.51.0`** 버전이 필요합니다[1][2]. 이전 버전을 사용하면 다음과 같은 오류가 발생합니다:

```
KeyError: 'qwen3'
```

## 🧠 **Qwen3 토크나이저의 특별한 기능**

### **Thinking Mode 지원**
Qwen3 토크나이저는 **하이브리드 사고 모드**를 지원하는 특별한 기능이 있습니다[1][3]:

```python
# Thinking Mode 활성화 (기본값)
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
    enable_thinking=True  # 사고 모드 활성화
)

# Non-Thinking Mode 활성화
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
    enable_thinking=False  # 빠른 응답 모드
)
```

### **사고 내용 파싱 기능**
Qwen3 토크나이저는 `` 태그를 통해 사고 과정을 분리할 수 있습니다[1][4]:

```python
# 사고 내용과 실제 응답 분리
try:
    # 151668은  토큰 ID
    index = len(output_ids) - output_ids[::-1].index(151668)
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True)
content = tokenizer.decode(output_ids[index:], skip_special_tokens=True)
```

## 🌐 **다국어 지원**

Qwen3 토크나이저는 **119개 언어 및 방언**을 지원합니다[3][5]. 이는 기존 토크나이저보다 훨씬 광범위한 언어 커버리지를 제공합니다.

## 📊 **모델별 토크나이저 사용**

모든 Qwen3 모델 버전에서 동일한 토크나이저를 사용합니다:

| 모델 | 토크나이저 |
|------|-----------|
| **Qwen3-0.6B** | `Qwen/Qwen3-0.6B` |
| **Qwen3-8B** | `Qwen/Qwen3-8B` |
| **Qwen3-30B-A3B** | `Qwen/Qwen3-30B-A3B` |
| **Qwen3-235B-A22B** | `Qwen/Qwen3-235B-A22B` |

## ⚠️ **중요한 주의사항**

### **전용 토크나이저 필수 사용**
Qwen3는 다른 모델의 토크나이저와 **호환되지 않습니다**. 반드시 해당 Qwen3 모델의 전용 토크나이저를 사용해야 합니다.

### **특수 명령어 지원**
Qwen3 토크나이저는 사용자 입력에서 `/think`와 `/no_think` 명령어를 인식합니다[1][6]:

```python
# 사고 모드 강제 활성화
user_input = "복잡한 수학 문제를 풀어줘 /think"

# 빠른 응답 모드 강제 활성화  
user_input = "간단한 번역 해줘 /no_think"
```

결론적으로, Qwen3를 사용할 때는 **반드시 Qwen3 전용 토크나이저**를 사용해야 하며, 이는 모델의 고유한 사고 모드 기능과 다국어 지원을 위해 특별히 설계된 토크나이저입니다.



Gemma 3는 **Google에서 개발한 최신 멀티모달, 다국어 지원 대화형 AI 모델**입니다. 텍스트 생성과 이미지 이해 작업에 모두 특화되어 있으며, 질문 답변, 요약, 추론 등 다양한 작업에 적합합니다[1].

## 🚀 **Gemma 3의 주요 특징**

### **멀티모달 및 다국어 지원**
Gemma 3는 **텍스트와 이미지를 동시에 처리**할 수 있는 멀티모달 모델입니다[1]. 또한 **향상된 다국어 기능**을 제공하며, 특히 중국어, 일본어, 한국어 텍스트 인코딩이 크게 개선되었습니다[2].

### **개선된 토크나이저**
Gemma 3는 **262K 어휘 크기의 SentencePiece 토크나이저**를 사용합니다[2][3]. 이는 Gemini와 동일한 토크나이저로, 비영어권 언어에 대해 더 균형 잡힌 성능을 제공합니다[3].

## 🔧 **사용 방법**

### **기본 설치 및 설정**
Gemma 3를 사용하려면 **`transformers>=4.50.0`** 버전이 필요합니다[1]:

```bash
pip install -U transformers
```

### **Pipeline API 사용**
```python
from transformers import pipeline
import torch

pipe = pipeline("text-generation", model="google/gemma-3-1b-it", device="cuda", torch_dtype=torch.bfloat16)

messages = [
    {
        "role": "system",
        "content": [{"type": "text", "text": "You are a helpful assistant."}]
    },
    {
        "role": "user", 
        "content": [{"type": "text", "text": "Write a poem on Hugging Face, the company"}]
    }
]

output = pipe(messages, max_new_tokens=50)
```

### **직접 모델 사용**
```python
from transformers import AutoTokenizer, Gemma3ForCausalLM
import torch

model_id = "google/gemma-3-1b-it"
model = Gemma3ForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_id)

messages = [
    {
        "role": "system",
        "content": [{"type": "text", "text": "You are a helpful assistant."}]
    },
    {
        "role": "user",
        "content": [{"type": "text", "text": "Who are you?"}]
    }
]

inputs = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    return_tensors="pt"
).to(model.device)

with torch.inference_mode():
    outputs = model.generate(**inputs, max_new_tokens=64)
    response = tokenizer.batch_decode(outputs)
```

## 📊 **모델 버전**

Gemma 3는 다양한 크기의 모델을 제공합니다:

- **Gemma 3-1B**: 경량화된 버전
- **Gemma 3-4B**: 중간 크기 모델  
- **텍스트 전용 모델**: `google/gemma-3-1b-pt`[4]
- **인스트럭션 튜닝 모델**: `google/gemma-3-1b-it`[1]

## 🎯 **주요 개선사항**

### **토크나이저 업그레이드**
기존 Gemma 모델과 달리 Gemma 3는 **새로운 토크나이저를 반드시 사용**해야 합니다[3]. 이는 오류를 방지하고 향상된 다국어 성능을 제공하기 위함입니다.

### **멀티모달 처리**
이미지와 텍스트를 함께 처리할 때는 **`` 토큰**을 사용하며, 고해상도 이미지의 경우 `do_pan_and_scan=True` 옵션을 통해 성능을 향상시킬 수 있습니다[4].

Gemma 3는 Google의 최신 AI 기술이 집약된 모델로, 다양한 언어와 모달리티를 지원하는 강력한 대화형 AI 솔루션입니다.


# 📋 GitHub 리포지토리 상단 메뉴 기능 완전 가이드

GitHub 리포지토리에 들어가면 상단에 여러 탭 메뉴들이 나타납니다. 각각의 기능을 자세히 설명해드리겠습니다!

## 🏠 **Code (코드)**

### **주요 기능**
- **파일 브라우저**: 리포지토리의 모든 파일과 폴더 구조를 탐색[1]
- **코드 네비게이션**: 함수나 메서드의 정의로 바로 이동하거나 참조를 찾을 수 있음[1]
- **브랜치 선택**: 다른 브랜치의 코드를 확인 가능
- **파일 생성/업로드**: 새 파일 생성 또는 기존 파일 업로드[2]

### **지원 언어**
코드 네비게이션은 다음 언어들을 지원합니다[1]:
- Bash, C, C#, C++, Go, Java, JavaScript, Python, Ruby, Rust 등

## 🔍 **Issues (이슈)**

### **주요 기능**
- **버그 리포트**: 발견된 버그나 문제점 보고
- **기능 요청**: 새로운 기능에 대한 제안
- **토론**: 프로젝트 관련 논의 진행
- **작업 추적**: 해야 할 일들을 체계적으로 관리

### **특징**
- **라벨링**: 이슈를 카테고리별로 분류
- **담당자 지정**: 특정 개발자에게 이슈 할당
- **마일스톤**: 버전별 목표 설정

## 🔄 **Pull Requests (풀 리퀘스트)**

### **주요 기능**
- **코드 리뷰**: 다른 개발자의 코드 변경사항 검토
- **병합 요청**: 브랜치의 변경사항을 메인 브랜치에 합치기
- **토론**: 코드 변경에 대한 의견 교환
- **승인 프로세스**: 코드 품질 관리

### **워크플로우**
1. 브랜치에서 작업 완료
2. Pull Request 생성
3. 코드 리뷰 진행
4. 승인 후 메인 브랜치에 병합

## ⚡ **Actions (액션)**

### **주요 기능**
- **CI/CD**: 자동화된 빌드, 테스트, 배포
- **워크플로우**: 코드 변경 시 자동 실행되는 작업들
- **자동화**: 반복적인 작업들을 자동으로 처리

### **활용 예시**
- 코드 푸시 시 자동 테스트 실행
- 릴리스 시 자동 배포
- 코드 품질 검사

## 📊 **Projects (프로젝트)**

### **주요 기능**
- **칸반 보드**: 작업 진행 상황을 시각적으로 관리
- **로드맵**: 프로젝트 일정 계획
- **작업 추적**: 이슈와 PR을 프로젝트 보드에서 관리

### **보드 유형**
- **To Do**: 해야 할 작업
- **In Progress**: 진행 중인 작업
- **Done**: 완료된 작업

## 🔒 **Security (보안)**

### **주요 기능**
- **보안 취약점**: 코드의 보안 문제점 탐지
- **의존성 검사**: 사용 중인 라이브러리의 보안 상태 확인
- **보안 정책**: 보안 관련 가이드라인 설정
- **보안 알림**: 취약점 발견 시 자동 알림

## 📈 **Insights (인사이트)**

### **주요 기능**
- **기여자 통계**: 누가 얼마나 기여했는지 확인
- **커밋 활동**: 시간별, 날짜별 커밋 패턴 분석
- **코드 빈도**: 언어별 코드 사용량 통계
- **네트워크 그래프**: 브랜치와 포크 관계 시각화

### **분석 데이터**
- 커밋 수, 추가/삭제된 라인 수
- 기여자별 활동량
- 프로젝트 성장 추이

## ⚙️ **Settings (설정)**

### **주요 기능**
- **리포지토리 설정**: 이름, 설명, 가시성 변경[3][4]
- **브랜치 보호**: 중요한 브랜치에 대한 보호 규칙 설정[4]
- **협업자 관리**: 다른 사용자에게 접근 권한 부여[2]
- **웹훅**: 외부 서비스와 연동 설정

### **중요 설정들**
- **Public/Private**: 리포지토리 공개 여부[2][4]
- **Branch Protection**: 메인 브랜치 보호 설정[4]
- **Code Owners**: 코드 소유자 지정[4]

## 🎯 **추가 기능들**

### **상단 우측 버튼들**
- **⭐ Star**: 리포지토리를 즐겨찾기에 추가
- **👁️ Watch**: 리포지토리 활동 알림 받기
- **🍴 Fork**: 리포지토리를 내 계정으로 복사
- **📥 Code**: 클론 URL 및 다운로드 옵션[2]

### **네비게이션 개선사항**
GitHub는 2023년 10월부터 **새로운 네비게이션 디자인**을 적용했습니다[5]:
- **효율적인 브레드크럼**: 현재 위치를 명확히 표시
- **간소화된 메뉴**: 빠른 접근을 위한 메뉴 구조
- **검색 최적화**: 코드 검색 경험 향상
- **접근성 개선**: 다양한 기기와 보조 기술 지원

